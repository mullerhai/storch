#  Build Powerful AI Infrastructure for Scala 3 ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ
# .                        ğŸ–ï¸ AI Infra 3.0 ON Scala3 ! ğŸ–ï¸
# STorch AI - ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ GPU Accelerated Distributeed Deep Learning && LLM && RL for Scala 3 ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ

## STorch is a Scala library for fast tensor computations and deep learning, same as PyTorch.
![storch-title](./docs/images/title.png) 
### Like PyTorch, STorch provides ğŸ“‡ ğŸ  ğŸªŸ ğŸ ğŸ§
* Both Compile from Cpp LibTorch, STorch has Same Equal Position with Pytorch
* A NumPy like API for working with tensors 
* GPU support [Future will support HuaWei CANN NPU]
* Automatic differentiation on Linux | Mac | Windows | IOS | Andriod
* A neural network API for building and training neural networks.
* What All you need Pytorch features could find in STorch !!!
* Minimal time and learning cost migrate from python pytorch developer
* Maximal ROI for Big Data developer and Data Scientist
* Support LLM | Recommend System | Compute Vision | NLP | Financial Tech | RL |Big Data | Data Science Research
* Big Deep Learning Environment Chains boost you concentrate on  development
* Support Java SMID Vector API, Please use JDK Version >= 17
* Could Use NCCL | GLOO | RAY | SPARK | FLINK for DISTRIBUTE TRAINING and inference [Future will support UCC | MPI] 
* Best Data Driven Process and Monad Feature Engineer Model Pipeline Partner for Apache Spark | Flink  | Polar

STorch aims to close to the Python API to make porting existing models and the life of people already familiar with PyTorch easier.

STorch Tutorial ,see here -> https://github.com/mullerhai/storch-tutorial

### STorch LLM -> Scala3gpt-llm is ready to use Pure Pytorch Neural Network for Scala 3 ON LLM

![storch-action](./docs/images/action.png)
### How to fetch STorch Library [Extended Version]
```scala 3

libraryDependencies +=   "io.github.mullerhai" % "storch_core_3" % "0.6.2-1.15.2"

```


# STorch AI Family:
## STorch Numpy
## STorch Pandas
## STorch Tensorboard
## STorch OpenCV
## STorch Ffmpeg
## STorch Librosa
## STorch Spark
## STorch Flink
## STroch Cuda
## STorch TensorRT
## STorch Triton
## STorch Transformers
## Storch Multinmodel
## STorch RL
## STorch Data
## STorch Graph
## STorch Geometric
## STorch Peft
## STorch Vision
## STorch Text
## STorch Audio
## STorch Jieba
## STorch Recommend
## STorch Finance
## STorch Drug
## STorch LLM
## STorch Faiss
## STorch Milvus
## STorch CSV | Pickle | Hdf5 | Parquet | Orc | Avro | Json | Xml | Sql | Hive | Kafka | Redis | Mysql | Postgres | Sqlite | Clickhouse | MongoDB | Cassandra | Redis | Kafka | RabbitMQ | Celery | Polars | Scikit-learn | TensorRT | Triton | Transformers | NLP | Computer Vision | Recommend System | Financial Tech | RL | Big Data | Data Science Research


### How to learn STorch
## STorch Tutorial


For documentation, see https://storch.dev

## Example:

```scala
val data = Seq(0,1,2,3)
// data: Seq[Int] = List(0, 1, 2, 3)
val t1 = torch.Tensor(data)
// t1: Tensor[Int32] = dtype=int32, shape=[4], device=CPU 
// [0, 1, 2, 3]
t1.equal(torch.arange(0,4))
// res0: Boolean = true
val t2 = t1.to(dtype=float32)
// t2: Tensor[Float32] = dtype=float32, shape=[4], device=CPU 
// [0,0000, 1,0000, 2,0000, 3,0000]
val t3 = t1 + t2
// t3: Tensor[Float32] = dtype=float32, shape=[4], device=CPU 
// [0,0000, 2,0000, 4,0000, 6,0000]

val shape = Seq(2l,3l)
// shape: Seq[Long] = List(2, 3)
val randTensor = torch.rand(shape)
// randTensor: Tensor[Float32] = dtype=float32, shape=[2, 3], device=CPU 
// [[0,4341, 0,9738, 0,9305],
//  [0,8987, 0,1122, 0,3912]]
val zerosTensor = torch.zeros(shape, dtype=torch.int64)
// zerosTensor: Tensor[Int64] = dtype=int64, shape=[2, 3], device=CPU 
// [[0, 0, 0],
//  [0, 0, 0]]

val x = torch.ones(Seq(5))
// x: Tensor[Float32] = dtype=float32, shape=[5], device=CPU 
// [1,0000, 1,0000, 1,0000, 1,0000, 1,0000]
val w = torch.randn(Seq(5, 3), requiresGrad=true)
// w: Tensor[Float32] = dtype=float32, shape=[5, 3], device=CPU 
// [[0,8975, 0,5484, 0,2307],
//  [0,2689, 0,7430, 0,6446],
//  [0,9503, 0,6342, 0,7523],
//  [0,5332, 0,7497, 0,3665],
//  [0,3376, 0,6040, 0,5033]]
val b = torch.randn(Seq(3), requiresGrad=true)
// b: Tensor[Float32] = dtype=float32, shape=[3], device=CPU 
// [0,2638, 0,9697, 0,3664]
val z = (x matmul w) + b
// z: Tensor[Float32] = dtype=float32, shape=[3], device=CPU 
// [3,2513, 4,2490, 2,8640]
```



## STorch Build LLM 

```scala 3

package moe

import torch.{Float32,Int64,Int32, *}
import torch.nn.*
import torch.nn.functional as F
import torch.nn.modules.{HasParams, TensorModule}
import torch.optim.{Adam, Optimizer}

import scala.util.Random

 set random seed 
Random.setSeed(1024)
torch.manualSeed(1024)


class BasicExpert[ParamType <: FloatNN: Default](featureIn: Int, featureOut: Int) extends HasParams[ParamType]
with TensorModule[ParamType]  {
  val linear = register(nn.Linear(featureIn, featureOut))
  def apply(x: Tensor[ParamType]): Tensor[ParamType] = {
    linear.forward(x)
  }
  def forward(x: Tensor[ParamType]): Tensor[ParamType] = {
    linear.forward(x)
  }
}

 
class BasicMOE[ParamType <: FloatNN: Default](featureIn: Int, featureOut: Int, expertNumber: Int) extends HasParams[ParamType]
  with TensorModule[ParamType]  {
  val experts = nn.ModuleList((0 until expertNumber).map(num => new BasicExpert(featureIn, featureOut))*)
  val gate = register(Linear(featureIn, expertNumber))
  override def apply(v1: Tensor[ParamType]): Tensor[ParamType] = ???
  def forward(x: Tensor[ParamType]): Tensor[ParamType] = {
    // x çš„å½¢çŠ¶æ˜¯ (batch, featureIn)
    val expertWeight = gate.forward(x)  // å½¢çŠ¶æ˜¯ (batch, expertNumber)
    // è®¡ç®—æ¯ä¸ªä¸“å®¶çš„è¾“å‡ºå¹¶å¢åŠ ä¸€ä¸ªç»´åº¦
    val expertOutList = experts.map(expert => expert(x).unsqueeze(1))
    // æ‹¼æ¥ä¸“å®¶è¾“å‡ºï¼Œå½¢çŠ¶å˜ä¸º (batch, expertNumber, featureOut)
    val expertOutput = torch.cat(expertOutList.toSeq, dim = 1)
    // è°ƒæ•´æƒé‡å½¢çŠ¶ä»¥è¿›è¡ŒçŸ©é˜µä¹˜æ³•
    val reshapedWeight = expertWeight.unsqueeze(1)  // (batch, 1, expertNumber)

    // çŸ©é˜µä¹˜æ³•è®¡ç®—æœ€ç»ˆè¾“å‡º
    val output = reshapedWeight.matmul(expertOutput)  // (batch, 1, featureOut)

    // ç§»é™¤å¤šä½™çš„ç»´åº¦
    output.squeeze()
  }
}

 test BasicMOE
object TestBasicMOE {
  def apply(): Unit = {
    val x = torch.rand(Seq(2, 4))
    val basicMoe = BasicMOE(4, 3, 2)
    val out = basicMoe.forward(x)
    println(out)
  }
}

 MOE Router
class MOERouter[ParamType <: FloatNN: Default](hiddenDim: Int, expertNumber: Int, topK: Int) extends HasParams[ParamType]
  with TensorModule[ParamType]  {
  val gate = register(nn.Linear(hiddenDim, expertNumber))
  override def apply(v1: Tensor[ParamType]): Tensor[ParamType] = ???
  def forward(hiddenStates: Tensor[ParamType]): (Tensor[ParamType], Tensor[ParamType], Tensor[Int64], Tensor[ParamType]) = {
    // è®¡ç®—è·¯ç”±logits
    val routerLogits = gate.forward(hiddenStates)  // å½¢çŠ¶æ˜¯ (b * s, expertNumber)
    // è®¡ç®—ä¸“å®¶ç»è¿‡softmaxä¹‹åçš„æ¦‚ç‡
    val routingProbs = F.softmax(routerLogits, dim = -1, dtype = hiddenStates.dtype)
    // è®¡ç®—topkçš„ä¸“å®¶çš„è¾“å‡º
    val routerWeights_selectedExperts = torch.topk(routingProbs, topK, dim = -1) //, largest = true, sorted = true
    val (routerWeights, selectedExperts) = (routerWeights_selectedExperts._1,routerWeights_selectedExperts._2)
    // ä¸“å®¶æƒé‡å½’ä¸€åŒ–
    val normalizedWeights = routerWeights / routerWeights.sum(dim = -1, keepdim = true)
    val routerWeightsTyped = normalizedWeights.to(hiddenStates.dtype)
    // ç”Ÿæˆä¸“å®¶æ©ç 
    val expertMask = F.one_hot(selectedExperts, numClasses = expertNumber)
    val permutedMask = expertMask.permute(2, 1, 0).to(hiddenStates.dtype)
    (routerLogits, routerWeightsTyped, selectedExperts, permutedMask)
  }
}

 MOE Router Config
case class MOEConfig(
    hiddenDim: Int,
    expertNumber: Int,
    topK: Int,
    sharedExpertsNumber: Int = 2
)

 Sparse MOE Model
class SparseMOE[ParamType <: FloatNN: Default](config: MOEConfig) extends HasParams[ParamType]
  with TensorModule[ParamType] {
  val hiddenDim: Int = config.hiddenDim
  val expertNumber: Int = config.expertNumber
  val topK: Int = config.topK
  val experts = nn.ModuleList( (0 until expertNumber).map(num => new BasicExpert(hiddenDim, hiddenDim))* )
  val router = MOERouter(hiddenDim, expertNumber, topK)
  override def apply(v1: Tensor[ParamType]): Tensor[ParamType] = ???
  def forward(x: Tensor[ParamType]): (Tensor[ParamType], Tensor[ParamType]) = {
    // x å½¢çŠ¶æ˜¯ (b, s, hiddenDim)
    val batchSize = x.shape(0).toInt
    val seqLen = x.shape(1).toInt
    // åˆå¹¶å‰ä¸¤ä¸ªç»´åº¦ï¼Œå› ä¸ºä¸æ˜¯Sampleç»´åº¦äº†ï¼Œè€Œæ˜¯tokenç»´åº¦
    val hiddenStates = x.view(-1, hiddenDim) // å½¢çŠ¶æ˜¯(b * s, hiddenDim)
    val (routerLogits, routerWeights, selectedExpertsIndices, expertMask) = 
      router.forward(hiddenStates)
    // åˆ›å»ºè¾“å‡ºå¼ é‡
    val finalHiddenStates = torch.zeros(
      Seq(batchSize * seqLen, hiddenDim),
      dtype = hiddenStates.dtype,
      device = hiddenStates.device
    )
    // å¯¹æ¯ä¸ªä¸“å®¶è¿›è¡Œå¤„ç†
//    hiddenStates.toArray
    for (expertIdx <- 0 until expertNumber) {
      val expertLayer = experts(expertIdx)
      // è·å–å½“å‰ä¸“å®¶çš„æ©ç å¹¶æ‰¾åˆ°éœ€è¦å¤„ç†çš„token
      val idx_topx = torch.where(expertMask(expertIdx))
      val idx = idx_topx(0)
      val topX: Tensor[ParamType] = idx_topx(1)
//      topX.numpy().toArray
//      val (idx, topX) = torch.where(expertMask(expertIdx))
      val hiddenStateUnsqueezed = hiddenStates.unsqueeze(0)
      // æå–éœ€è¦å¤„ç†çš„tokençš„éšè—çŠ¶æ€
//      val currentState = hiddenStateUnsqueezed(::,topX.toArray.toSeq.asInstanceOf[Seq[Long]], ::).reshape(-1, hiddenDim)
      val currentState = hiddenStateUnsqueezed(::, topX.to(DType.int64), ::).reshape(-1, hiddenDim)
      // åº”ç”¨ä¸“å®¶å±‚å¹¶åŠ æƒ
      val weights = routerWeights(topX.to(DType.int64), idx.to(DType.int64)).unsqueeze(-1)
      val currentHiddenStates = expertLayer(currentState) * weights

      // å°†å½“å‰ä¸“å®¶çš„è¾“å‡ºåŠ åˆ°æœ€ç»ˆç»“æœä¸­
      finalHiddenStates.index_add_(0, topX.to(DType.int64), currentHiddenStates.to(hiddenStates.dtype))
    }
    // å°†ç»“æœè¿˜åŸåˆ°åŸå§‹å½¢çŠ¶
    val reshapedOutput = finalHiddenStates.reshape(batchSize, seqLen, hiddenDim)

    (reshapedOutput, routerLogits)
  }
}


            current_state = hidden_states.unsqueeze(
                0
            )[:, top_x, :].reshape(-1, hidden_dim) # ï¼ˆselected_token_number, hidden_dimï¼‰

            # router_weight çš„ shape æ˜¯ (b * s, top_k)
            current_hidden_states = expert_layer(
                current_state
            ) * router_weights[top_x, idx].unsqueeze(-1)  # ï¼ˆselected_token_number, 1ï¼‰ è¿™é‡Œæœ‰å¹¿æ’­

 test Sparse MOE model
object TestTokenLevelMOE {
  def apply(): Unit = {
    val x = torch.rand(Seq(2, 4, 16))
    val config = MOEConfig(16, 2, 2)
    val tokenLevelMoe = SparseMOE(config)
    val (out, logits) = tokenLevelMoe.forward(x)
    println(s"Output shape: ${out.shape}, Router logits shape: ${logits.shape}")
  }
}

 Share expert's Sparse MOE model
class ShareExpertMOE[ParamType <: FloatNN: Default](config: MOEConfig) extends HasParams[ParamType]
  with TensorModule[ParamType] {
  val moeModel = SparseMOE(config)
  val sharedExperts = nn.ModuleList(
    (0 until config.sharedExpertsNumber).map(num => BasicExpert(config.hiddenDim, config.hiddenDim))*
  )
  override def apply(v1: Tensor[ParamType]): Tensor[ParamType] = ???
  def forward(x: Tensor[ParamType]): (Tensor[ParamType], Tensor[ParamType]) = {
    // é¦–å…ˆé€šè¿‡moeæ¨¡å‹
    val (sparseMoeOut, routerLogits) = moeModel.forward(x)

    // ç„¶åé€šè¿‡å…±äº«ä¸“å®¶
    val sharedExpertsOut = sharedExperts.map(expert => expert(x))

    // å †å å…±äº«ä¸“å®¶çš„è¾“å‡ºå¹¶æ±‚å’Œ
    val sharedExpertsOutSum = torch.stack(sharedExpertsOut.toSeq, dim = 0).sum(dim = 0)

    // å°†sparse_moe_outå’Œshared_experts_outç›¸åŠ 
    (sparseMoeOut + sharedExpertsOutSum, routerLogits)
  }
}

 test share expert's Sparse MOE model
object TestShareExpertMOE {
  def apply(): Unit = {
    val x = torch.rand(Seq(2, 4, 16))
    val config = MOEConfig(16, 2, 2)
    val shareExpertMoe = ShareExpertMOE(config)
    val (out, logits) = shareExpertMoe.forward(x)
    println(s"Output shape: ${out.shape}, Router logits shape: ${logits.shape}")
  }
}

// compute Switch Transformers's load balancing loss
def switchLoadBalancingLoss[ParamType <: FloatNN: Default](routerLogits: Tensor[ParamType], numExperts: Int): Tensor[Float32] = {
  // è®¡ç®—è·¯ç”±æ¦‚ç‡
  val routerProbs = torch.softmax(routerLogits, dim = -1)  // [b*s, numExperts]

  // è·å–æ¯ä¸ªtokençš„æœ€ä¼˜ä¸“å®¶
  val anySelectedExperts = torch.topk(routerProbs, k = 2, dim = -1, largest = true, sorted = true)  // [b*s]

  // åˆ›å»ºone-hotçŸ©é˜µè¡¨ç¤ºé€‰ä¸­çš„ä¸“å®¶
  val mask = F.one_hot(anySelectedExperts._2.to(DType.int64), numExperts).float()  // [b*s, numExperts]

  // è®¡ç®—æ¯ä¸ªä¸“å®¶çš„æœŸæœ›è´Ÿè½½ (ç†æƒ³æƒ…å†µä¸‹åº”è¯¥æ˜¯ 1/numExperts)
  val expectedLoad = torch.onesLike(routerProbs) / numExperts

  // è®¡ç®—å®é™…è´Ÿè½½ (æ¯ä¸ªä¸“å®¶å¤„ç†çš„tokenæ•°é‡é™¤ä»¥æ€»tokenæ•°é‡)
  // åœ¨batchç»´åº¦ä¸Šè®¡ç®—å¹³å‡å€¼
  val actualLoad = mask.mean(dim = 0)  // [numExperts]

  // è®¡ç®—auxiliary loss
  // è¿™ä¼šæƒ©ç½šè´Ÿè½½åˆ†å¸ƒä¸æœŸæœ›è´Ÿè½½çš„å·®å¼‚
  val auxLoss = torch.sum(actualLoad * routerProbs.mean(dim = 0)) * numExperts

  // è®¡ç®—z_loss (å¯é€‰)
  // è¿™ä¼šæƒ©ç½šè¿‡å¤§çš„è·¯ç”±logits
  val zLoss = torch.mean(torch.square(routerLogits))
  val zLossWeight = 0.001f  // å¯è°ƒæ•´çš„è¶…å‚æ•°

  // æ€»æŸå¤±
  val totalLoss = auxLoss + zLoss * zLossWeight

  totalLoss.to(torch.float32)
}

// test MOE training 
object TestMOETraining {
  def apply(): Unit = {
    // åˆ›å»ºç®€å•çš„æ•°æ®é›†å‚æ•°
    val batchSize = 32
    val seqLen = 16
    val hiddenDim = 32
    val numBatches = 100

    // åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
    val config = MOEConfig(
      hiddenDim = hiddenDim,
      expertNumber = 4,
      topK = 2,
      sharedExpertsNumber = 2
    )

    val model = ShareExpertMOE(config)
    val optimizer = Adam(model.parameters(true), lr = 0.001f)

    // è®­ç»ƒå¾ªç¯
    model.train()
    for (batch <- 0 until numBatches) {
      // ç”Ÿæˆéšæœºè¾“å…¥æ•°æ®
      val x = torch.randn(Seq(batchSize, seqLen, hiddenDim))
      val target = torch.randn(Seq(batchSize, seqLen, hiddenDim))

      // å‰å‘ä¼ æ’­
      val (output, routerLogits) = model.forward(x)

      // è®¡ç®—æŸå¤±
      // é¢„æµ‹çš„MSEæŸå¤±
      val mseLoss = F.mse_loss(output, target)

      val auxLoss = switchLoadBalancingLoss(routerLogits, config.expertNumber)
      // ç»„åˆæŸå¤±
      val totalLoss = mseLoss + 0.01f * auxLoss

      // åå‘ä¼ æ’­å’Œä¼˜åŒ–
      optimizer.zeroGrad()
      totalLoss.backward()
      optimizer.step()

      if (batch % 10 == 0) {
        println(f"Batch $batch, Loss: ${totalLoss.item} " +
                f"(MSE: ${mseLoss.item}, Aux: ${auxLoss.item})")
      }
    }
  }
}

// run all tests
object MOETests {
  def main(args: Array[String]): Unit = {
    println("Testing Basic MOE:")
    TestBasicMOE()

    println("\nTesting Token Level MOE:")
    TestTokenLevelMOE()

    println("\nTesting Share Expert MOE:")
    TestShareExpertMOE()

    println("\nTesting MOE Training:")
    TestMOETraining()
  }
}



```


![storch-1](./docs/images/storch-1.png)
![storch-2](./docs/images/storch-2.png)
![storch-3](./docs/images/storch-3.png)
![storch-4](./docs/images/storch-4.png)
![storch-5](./docs/images/storch-5.png)
![storch-6](./docs/images/storch-6.png)
![storch-7](./docs/images/storch-7.png)